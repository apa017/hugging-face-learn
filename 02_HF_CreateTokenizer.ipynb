{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Git Hub: Create Your Own Tokenizer\n",
    "\n",
    "In this notebook we will create a custom tokenizer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Your Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e75eebe386324abbae8f703a8c083142",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/512 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "03ac32042603436d9728fcbeb0d01aeb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00000-of-00001.parquet:   0%|          | 0.00/150k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd628084f2d54c619185ed5343c3c989",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "validation-00000-of-00001.parquet:   0%|          | 0.00/30.4k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af74315c1fb84288bc7c30434642c381",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "test-00000-of-00001.parquet:   0%|          | 0.00/39.2k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba2e9b63501f4a01b75c203a30970856",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/462 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb5a013c3f934a1bb92989468828d22c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/58 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4cf2e8ac32e0446fac7eb69843909ea3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/58 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# we will load our own dataset\n",
    "dataset = load_dataset('Kain17/reuters_articles')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['title', 'body'],\n",
       "        num_rows: 462\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['title', 'body'],\n",
       "        num_rows: 58\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['title', 'body'],\n",
       "        num_rows: 58\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modify the Existing Dataset\n",
    "\n",
    "We can perform changes on existing dataset before feeding it to the tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function\n",
    "def create_full_article_column(example):\n",
    "    return {\n",
    "        'full_article': \n",
    "        f\"TITLE : {example['title']}\\n\\nBODY : {example['body']}\"\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e5c1cb2b1daa4905b17ad55d06e6f6d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/462 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e60124a5ba4f4e469d43bbdb75af02d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/58 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b43ff88dd12459bbc8bb57efe3dfd13",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/58 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['title', 'body', 'full_article'],\n",
       "        num_rows: 462\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['title', 'body', 'full_article'],\n",
       "        num_rows: 58\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['title', 'body', 'full_article'],\n",
       "        num_rows: 58\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Map every column to create a full article column\n",
    "dataset = dataset.map(create_full_article_column)\n",
    "\n",
    "# Verify a new column `full_article` was created for each subset within the dataset\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'TITLE : HUGE OIL PLATFORMS DOT GULF LIKE BEACONS\\n\\nBODY : Huge oil platforms dot the Gulf like\\nbeacons -- usually lit up like Christmas trees at night.\\n    One of them, sitting astride the Rostam offshore oilfield,\\nwas all but blown out of the water by U.S. Warships on Monday.\\n    The Iranian platform, an unsightly mass of steel and\\nconcrete, was a three-tier structure rising 200 feet (60\\nmetres) above the warm waters of the Gulf until four U.S.\\nDestroyers pumped some 1,000 shells into it.\\n    The U.S. Defense Department said just 10 pct of one section\\nof the structure remained.\\n    U.S. helicopters destroyed three Iranian gunboats after an\\nAmerican helicopter came under fire earlier this month and U.S.\\nforces attacked, seized, and sank an Iranian ship they said had\\nbeen caught laying mines.\\n    But Iran was not deterred, according to U.S. defense\\nofficials, who said Iranian forces used Chinese-made Silkworm\\nmissiles to hit a U.S.-owned Liberian-flagged ship on Thursday\\nand the Sea Isle City on Friday.\\n    Both ships were hit in the territorial waters of Kuwait, a\\nkey backer of Iraq in its war with Iran.\\n    Henry Schuler, a former U.S. diplomat in the Middle East\\nnow with CSIS said Washington had agreed to escort Kuwaiti\\ntankers in order to deter Iranian attacks on shipping.\\n    But he said the deterrence policy had failed and the level\\nof violence and threats to shipping had increased as a result\\nof U.S. intervention and Iran\\'s response.\\n    The attack on the oil platform was the latest example of a\\nU.S. \"tit-for-tat\" policy that gave Iran the initiative, said\\nHarlan Ullman, an ex-career naval officer now with CSIS.\\n    He said with this appraoch America would suffer \"the death\\nof one thousand cuts.\"\\n    But for the United States to grab the initiative\\nmilitarily, it must take warlike steps such as mining Iran\\'s\\nharbors or blockading the mouth of the Gulf through which its\\nshipping must pass, Schuler said.\\n    He was among those advocating mining as a means of bringing\\nIran to the neogtiating table. If vital supplies were cut off,\\nTehran could not continue the war with Iraq.\\n    Ullman said Washington should join Moscow in a diplomatic\\ninitiative to end the war and the superpowers should impose an\\narms embargo against Tehran if it refused to negotiate.\\n    He said the United States should also threaten to mine and\\nblockade Iran if it continued fighting and must press Iraq to\\nacknowledge responsibility for starting the war as part of a\\nsettlement.\\n    Iranian and Western diplomats say Iraq started the war by\\ninvading Iran\\'s territory in 1980. Iraq blames Iran for the\\noutbreak of hostilities, which have entailed World War I-style\\ninfantry attacks resulting in horrific casualties.\\n    Each side has attacked the others\\' shipping.\\n Reuter\\n\\x03'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Verify the content of the column\n",
    "dataset['train'][1]['full_article']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Tokenizer\n",
    "\n",
    "We will use Hugging Face transformers class to create batched datasets for training and an iterator object for a later usage when training the tokenizer.\n",
    "\n",
    "We will use **GPT-2** model to demonstrate this part and the next sections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the batched dataset using the `full_article` column\n",
    "corpus = (\n",
    "    dataset[\"train\"][i : i + 1000][\"full_article\"] for i in range(0, len(dataset[\"train\"]), 1000)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "437afbe22d9f49edb80d2e393de6dddb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0259d4ed52644c778e2b9e869bb83e95",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d390b63dbcfe47b1a8a54241636966c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a64e64186744bd9a075ba19d3a6fccf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b8b1e9602504e0491011136d99697ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/apavigli/.local/share/virtualenvs/hugging-face-learn-mHgvpzAo/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# train gpt-2 tokenizer\n",
    "old_tokenizer = AutoTokenizer.from_pretrained('gpt2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start Batch Training...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Batch Training complete.\n",
      "\n",
      "Execution Time: 0.87 seconds.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/apavigli/.local/share/virtualenvs/hugging-face-learn-mHgvpzAo/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "# pass the training corpus into the tokenizer (specify training corpus and vocabulary size - e.g. 52000)\n",
    "\n",
    "print('Start Batch Training...\\n')\n",
    "start_time = time.time()\n",
    "\n",
    "tokenizer = old_tokenizer.train_new_from_iterator(corpus, 52000)\n",
    "\n",
    "print('Batch Training complete.\\n')\n",
    "end_time = time.time()\n",
    "\n",
    "execution_time = end_time-start_time\n",
    "print(f\"Execution Time: {execution_time:.2f} seconds.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Text: \n",
      "\n",
      "TITLE : ANIMAL FEED SHIP ON FIRE AGAIN AT CHINESE PORT\n",
      "\n",
      "BODY : The Cyprus vessel Fearless, 31,841 tonnes\n",
      "dw, which was on fire, grounded then towed to Yantai, China, in\n",
      "August, had all its cargo reloaded but the cargo in the no. 3\n",
      "hold caught fire on October 15.\n",
      "    The fire was put out with salt water and water from the\n",
      "no.4 hold has spread over most of the cargo. Some water is also\n",
      "in the no.5 hold. Bottom patching was reported complete but\n",
      "only the no.4 starboard wing tank has been pumped out and\n",
      "remains dry. The engine room is flooded to about three metres.\n",
      "    The ship was originally loaded with 10,000 tonnes of animal\n",
      "feed.\n",
      " REUTER\n",
      "\u0003\n"
     ]
    }
   ],
   "source": [
    "# test sample\n",
    "print('Original Text: \\n')\n",
    "example = dataset['test'][2]['full_article']\n",
    "print(example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized Example:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['TIT',\n",
       " 'LE',\n",
       " 'Ġ:',\n",
       " 'ĠAN',\n",
       " 'IM',\n",
       " 'AL',\n",
       " 'ĠFE',\n",
       " 'ED',\n",
       " 'ĠSH',\n",
       " 'IP',\n",
       " 'ĠON',\n",
       " 'ĠFIRE',\n",
       " 'ĠAGA',\n",
       " 'IN',\n",
       " 'ĠAT',\n",
       " 'ĠCH',\n",
       " 'IN',\n",
       " 'ESE',\n",
       " 'ĠP',\n",
       " 'ORT',\n",
       " 'Ċ',\n",
       " 'Ċ',\n",
       " 'B',\n",
       " 'ODY',\n",
       " 'Ġ:',\n",
       " 'ĠThe',\n",
       " 'ĠCyprus',\n",
       " 'Ġvessel',\n",
       " 'ĠFear',\n",
       " 'less',\n",
       " ',',\n",
       " 'Ġ31',\n",
       " ',',\n",
       " '8',\n",
       " '41',\n",
       " 'Ġtonnes',\n",
       " 'Ċ',\n",
       " 'd',\n",
       " 'w',\n",
       " ',',\n",
       " 'Ġwhich',\n",
       " 'Ġwas',\n",
       " 'Ġon',\n",
       " 'Ġfire',\n",
       " ',',\n",
       " 'Ġgrounded',\n",
       " 'Ġthen',\n",
       " 'Ġtowed',\n",
       " 'Ġto',\n",
       " 'ĠY',\n",
       " 'ant',\n",
       " 'ai',\n",
       " ',',\n",
       " 'ĠChina',\n",
       " ',',\n",
       " 'Ġin',\n",
       " 'Ċ',\n",
       " 'August',\n",
       " ',',\n",
       " 'Ġhad',\n",
       " 'Ġall',\n",
       " 'Ġits',\n",
       " 'Ġcargo',\n",
       " 'Ġreload',\n",
       " 'ed',\n",
       " 'Ġbut',\n",
       " 'Ġthe',\n",
       " 'Ġcargo',\n",
       " 'Ġin',\n",
       " 'Ġthe',\n",
       " 'Ġno',\n",
       " '.',\n",
       " 'Ġ3',\n",
       " 'Ċ',\n",
       " 'hold',\n",
       " 'Ġcaught',\n",
       " 'Ġfire',\n",
       " 'Ġon',\n",
       " 'ĠOctober',\n",
       " 'Ġ15',\n",
       " '.',\n",
       " 'Ċ',\n",
       " 'Ġ',\n",
       " 'Ġ',\n",
       " 'Ġ',\n",
       " 'ĠThe',\n",
       " 'Ġfire',\n",
       " 'Ġwas',\n",
       " 'Ġput',\n",
       " 'Ġout',\n",
       " 'Ġwith',\n",
       " 'Ġsalt',\n",
       " 'Ġwater',\n",
       " 'Ġand',\n",
       " 'Ġwater',\n",
       " 'Ġfrom',\n",
       " 'Ġthe',\n",
       " 'Ċ',\n",
       " 'no',\n",
       " '.',\n",
       " '4',\n",
       " 'Ġhold',\n",
       " 'Ġhas',\n",
       " 'Ġspread',\n",
       " 'Ġover',\n",
       " 'Ġmost',\n",
       " 'Ġof',\n",
       " 'Ġthe',\n",
       " 'Ġcargo',\n",
       " '.',\n",
       " 'ĠSome',\n",
       " 'Ġwater',\n",
       " 'Ġis',\n",
       " 'Ġalso',\n",
       " 'Ċ',\n",
       " 'in',\n",
       " 'Ġthe',\n",
       " 'Ġno',\n",
       " '.',\n",
       " '5',\n",
       " 'Ġhold',\n",
       " '.',\n",
       " 'ĠBottom',\n",
       " 'Ġpatch',\n",
       " 'ing',\n",
       " 'Ġwas',\n",
       " 'Ġreported',\n",
       " 'Ġcomplete',\n",
       " 'Ġbut',\n",
       " 'Ċ',\n",
       " 'only',\n",
       " 'Ġthe',\n",
       " 'Ġno',\n",
       " '.',\n",
       " '4',\n",
       " 'Ġstar',\n",
       " 'board',\n",
       " 'Ġwing',\n",
       " 'Ġtank',\n",
       " 'Ġhas',\n",
       " 'Ġbeen',\n",
       " 'Ġpumped',\n",
       " 'Ġout',\n",
       " 'Ġand',\n",
       " 'Ċ',\n",
       " 'rem',\n",
       " 'ains',\n",
       " 'Ġdry',\n",
       " '.',\n",
       " 'ĠThe',\n",
       " 'Ġengine',\n",
       " 'Ġroom',\n",
       " 'Ġis',\n",
       " 'Ġflooded',\n",
       " 'Ġto',\n",
       " 'Ġabout',\n",
       " 'Ġthree',\n",
       " 'Ġmetres',\n",
       " '.',\n",
       " 'Ċ',\n",
       " 'Ġ',\n",
       " 'Ġ',\n",
       " 'Ġ',\n",
       " 'ĠThe',\n",
       " 'Ġship',\n",
       " 'Ġwas',\n",
       " 'Ġoriginally',\n",
       " 'Ġloaded',\n",
       " 'Ġwith',\n",
       " 'Ġ10',\n",
       " ',',\n",
       " '000',\n",
       " 'Ġtonnes',\n",
       " 'Ġof',\n",
       " 'Ġanimal',\n",
       " 'Ċ',\n",
       " 'feed',\n",
       " '.',\n",
       " 'Ċ',\n",
       " 'ĠRE',\n",
       " 'UT',\n",
       " 'ER',\n",
       " 'Ċ',\n",
       " 'ă']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test old tokenizer\n",
    "print('Tokenized Example:')\n",
    "test1 = old_tokenizer.tokenize(example)\n",
    "test1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['TITLE',\n",
       " 'Ġ:',\n",
       " 'ĠAN',\n",
       " 'IM',\n",
       " 'AL',\n",
       " 'ĠFE',\n",
       " 'ED',\n",
       " 'ĠSH',\n",
       " 'IP',\n",
       " 'ĠON',\n",
       " 'ĠFI',\n",
       " 'RE',\n",
       " 'ĠAG',\n",
       " 'AIN',\n",
       " 'ĠAT',\n",
       " 'ĠCH',\n",
       " 'INES',\n",
       " 'E',\n",
       " 'ĠP',\n",
       " 'ORT',\n",
       " 'Ċ',\n",
       " 'Ċ',\n",
       " 'BODY',\n",
       " 'Ġ:',\n",
       " 'ĠThe',\n",
       " 'ĠC',\n",
       " 'y',\n",
       " 'pr',\n",
       " 'us',\n",
       " 'Ġvessel',\n",
       " 'ĠF',\n",
       " 'ear',\n",
       " 'less',\n",
       " ',',\n",
       " 'Ġ31',\n",
       " ',',\n",
       " '8',\n",
       " '41',\n",
       " 'Ġtonnes',\n",
       " 'Ċ',\n",
       " 'd',\n",
       " 'w',\n",
       " ',',\n",
       " 'Ġwhich',\n",
       " 'Ġwas',\n",
       " 'Ġon',\n",
       " 'Ġfire',\n",
       " ',',\n",
       " 'Ġg',\n",
       " 'round',\n",
       " 'ed',\n",
       " 'Ġthen',\n",
       " 'Ġto',\n",
       " 'w',\n",
       " 'ed',\n",
       " 'Ġto',\n",
       " 'ĠY',\n",
       " 'ant',\n",
       " 'a',\n",
       " 'i',\n",
       " ',',\n",
       " 'ĠChina',\n",
       " ',',\n",
       " 'Ġin',\n",
       " 'Ċ',\n",
       " 'August',\n",
       " ',',\n",
       " 'Ġhad',\n",
       " 'Ġall',\n",
       " 'Ġits',\n",
       " 'Ġc',\n",
       " 'argo',\n",
       " 'Ġre',\n",
       " 'lo',\n",
       " 'ad',\n",
       " 'ed',\n",
       " 'Ġbut',\n",
       " 'Ġthe',\n",
       " 'Ġc',\n",
       " 'argo',\n",
       " 'Ġin',\n",
       " 'Ġthe',\n",
       " 'Ġno',\n",
       " '.',\n",
       " 'Ġ3',\n",
       " 'Ċ',\n",
       " 'hold',\n",
       " 'Ġcaught',\n",
       " 'Ġfire',\n",
       " 'Ġon',\n",
       " 'ĠOctober',\n",
       " 'Ġ15',\n",
       " '.',\n",
       " 'ĊĠĠĠ',\n",
       " 'ĠThe',\n",
       " 'Ġfire',\n",
       " 'Ġwas',\n",
       " 'Ġput',\n",
       " 'Ġout',\n",
       " 'Ġwith',\n",
       " 'Ġs',\n",
       " 'alt',\n",
       " 'Ġwater',\n",
       " 'Ġand',\n",
       " 'Ġwater',\n",
       " 'Ġfrom',\n",
       " 'Ġthe',\n",
       " 'Ċ',\n",
       " 'no',\n",
       " '.',\n",
       " '4',\n",
       " 'Ġhold',\n",
       " 'Ġhas',\n",
       " 'Ġsp',\n",
       " 're',\n",
       " 'ad',\n",
       " 'Ġover',\n",
       " 'Ġmost',\n",
       " 'Ġof',\n",
       " 'Ġthe',\n",
       " 'Ġc',\n",
       " 'argo',\n",
       " '.',\n",
       " 'ĠSome',\n",
       " 'Ġwater',\n",
       " 'Ġis',\n",
       " 'Ġalso',\n",
       " 'Ċ',\n",
       " 'in',\n",
       " 'Ġthe',\n",
       " 'Ġno',\n",
       " '.',\n",
       " '5',\n",
       " 'Ġhold',\n",
       " '.',\n",
       " 'ĠBott',\n",
       " 'om',\n",
       " 'Ġpat',\n",
       " 'ching',\n",
       " 'Ġwas',\n",
       " 'Ġreported',\n",
       " 'Ġcomplete',\n",
       " 'Ġbut',\n",
       " 'Ċ',\n",
       " 'only',\n",
       " 'Ġthe',\n",
       " 'Ġno',\n",
       " '.',\n",
       " '4',\n",
       " 'Ġst',\n",
       " 'arb',\n",
       " 'oard',\n",
       " 'Ġw',\n",
       " 'ing',\n",
       " 'Ġtank',\n",
       " 'Ġhas',\n",
       " 'Ġbeen',\n",
       " 'Ġpumped',\n",
       " 'Ġout',\n",
       " 'Ġand',\n",
       " 'Ċ',\n",
       " 'remains',\n",
       " 'Ġdry',\n",
       " '.',\n",
       " 'ĠThe',\n",
       " 'Ġengine',\n",
       " 'Ġro',\n",
       " 'om',\n",
       " 'Ġis',\n",
       " 'Ġflood',\n",
       " 'ed',\n",
       " 'Ġto',\n",
       " 'Ġabout',\n",
       " 'Ġthree',\n",
       " 'Ġmetres',\n",
       " '.',\n",
       " 'ĊĠĠĠ',\n",
       " 'ĠThe',\n",
       " 'Ġship',\n",
       " 'Ġwas',\n",
       " 'Ġori',\n",
       " 'gin',\n",
       " 'ally',\n",
       " 'Ġlo',\n",
       " 'ad',\n",
       " 'ed',\n",
       " 'Ġwith',\n",
       " 'Ġ10',\n",
       " ',',\n",
       " '000',\n",
       " 'Ġtonnes',\n",
       " 'Ġof',\n",
       " 'Ġan',\n",
       " 'im',\n",
       " 'al',\n",
       " 'Ċ',\n",
       " 'f',\n",
       " 'eed',\n",
       " '.',\n",
       " 'Ċ',\n",
       " 'ĠREUTER',\n",
       " 'Ċ',\n",
       " 'ă']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test: tokenizer\n",
    "test2 = tokenizer.tokenize(example)\n",
    "test2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenization by old tokenizer: 184\n",
      "Tokenization by new tokenizer: 203\n"
     ]
    }
   ],
   "source": [
    "# Compare results\n",
    "print(f'Tokenization by old tokenizer: {len(test1)}')\n",
    "print(f'Tokenization by new tokenizer: {len(test2)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Push New Tokenizer to Hugging Face Hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c4f832e4d08d4ba188e050173f602833",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "\n",
    "# Login to Hugging Face Hub\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/Kain17/gp2-reuters-tokenizer/commit/2c6e1775132d08002ba65df99d88286ccd76aaa2', commit_message='Upload tokenizer', commit_description='', oid='2c6e1775132d08002ba65df99d88286ccd76aaa2', pr_url=None, repo_url=RepoUrl('https://huggingface.co/Kain17/gp2-reuters-tokenizer', endpoint='https://huggingface.co', repo_type='model', repo_id='Kain17/gp2-reuters-tokenizer'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Store tokenizer to hub\n",
    "tokenizer.push_to_hub(\"gp2-reuters-tokenizer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using our Custom Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "17203bdbb6fb4235b2cf68b31172573e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/441 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b7cdf7157a54f3e95cacbd1a7b0be93",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/209k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b3b298a417b24d5d930c2f70a463dc46",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/119k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4513b75ae2914c869e19ab154ee40917",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/972k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69da7deba2944b5c8bf9d276761dcecc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/99.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load custom tokenizer\n",
    "my_tokenizer = AutoTokenizer.from_pretrained(\"Kain17/gp2-reuters-tokenizer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2TokenizerFast(name_or_path='Kain17/gp2-reuters-tokenizer', vocab_size=14184, model_max_length=1024, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>'}, clean_up_tokenization_spaces=False),  added_tokens_decoder={\n",
       "\t0: AddedToken(\"<|endoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
       "}"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TITLE : ANIMAL FEED SHIP ON FIRE AGAIN AT CHINESE PORT\n",
      "\n",
      "BODY : The Cyprus vessel Fearless, 31,841 tonnes\n",
      "dw, which was on fire, grounded then towed to Yantai, China, in\n",
      "August, had all its cargo reloaded but the cargo in the no. 3\n",
      "hold caught fire on October 15.\n",
      "    The fire was put out with salt water and water from the\n",
      "no.4 hold has spread over most of the cargo. Some water is also\n",
      "in the no.5 hold. Bottom patching was reported complete but\n",
      "only the no.4 starboard wing tank has been pumped out and\n",
      "remains dry. The engine room is flooded to about three metres.\n",
      "    The ship was originally loaded with 10,000 tonnes of animal\n",
      "feed.\n",
      " REUTER\n",
      "\u0003\n",
      "203\n",
      "['TITLE', 'Ġ:', 'ĠAN', 'IM', 'AL', 'ĠFE', 'ED', 'ĠSH', 'IP', 'ĠON', 'ĠFI', 'RE', 'ĠAG', 'AIN', 'ĠAT', 'ĠCH', 'INES', 'E', 'ĠP', 'ORT', 'Ċ', 'Ċ', 'BODY', 'Ġ:', 'ĠThe', 'ĠC', 'y', 'pr', 'us', 'Ġvessel', 'ĠF', 'ear', 'less', ',', 'Ġ31', ',', '8', '41', 'Ġtonnes', 'Ċ', 'd', 'w', ',', 'Ġwhich', 'Ġwas', 'Ġon', 'Ġfire', ',', 'Ġg', 'round', 'ed', 'Ġthen', 'Ġto', 'w', 'ed', 'Ġto', 'ĠY', 'ant', 'a', 'i', ',', 'ĠChina', ',', 'Ġin', 'Ċ', 'August', ',', 'Ġhad', 'Ġall', 'Ġits', 'Ġc', 'argo', 'Ġre', 'lo', 'ad', 'ed', 'Ġbut', 'Ġthe', 'Ġc', 'argo', 'Ġin', 'Ġthe', 'Ġno', '.', 'Ġ3', 'Ċ', 'hold', 'Ġcaught', 'Ġfire', 'Ġon', 'ĠOctober', 'Ġ15', '.', 'ĊĠĠĠ', 'ĠThe', 'Ġfire', 'Ġwas', 'Ġput', 'Ġout', 'Ġwith', 'Ġs', 'alt', 'Ġwater', 'Ġand', 'Ġwater', 'Ġfrom', 'Ġthe', 'Ċ', 'no', '.', '4', 'Ġhold', 'Ġhas', 'Ġsp', 're', 'ad', 'Ġover', 'Ġmost', 'Ġof', 'Ġthe', 'Ġc', 'argo', '.', 'ĠSome', 'Ġwater', 'Ġis', 'Ġalso', 'Ċ', 'in', 'Ġthe', 'Ġno', '.', '5', 'Ġhold', '.', 'ĠBott', 'om', 'Ġpat', 'ching', 'Ġwas', 'Ġreported', 'Ġcomplete', 'Ġbut', 'Ċ', 'only', 'Ġthe', 'Ġno', '.', '4', 'Ġst', 'arb', 'oard', 'Ġw', 'ing', 'Ġtank', 'Ġhas', 'Ġbeen', 'Ġpumped', 'Ġout', 'Ġand', 'Ċ', 'remains', 'Ġdry', '.', 'ĠThe', 'Ġengine', 'Ġro', 'om', 'Ġis', 'Ġflood', 'ed', 'Ġto', 'Ġabout', 'Ġthree', 'Ġmetres', '.', 'ĊĠĠĠ', 'ĠThe', 'Ġship', 'Ġwas', 'Ġori', 'gin', 'ally', 'Ġlo', 'ad', 'ed', 'Ġwith', 'Ġ10', ',', '000', 'Ġtonnes', 'Ġof', 'Ġan', 'im', 'al', 'Ċ', 'f', 'eed', '.', 'Ċ', 'ĠREUTER', 'Ċ', 'ă']\n"
     ]
    }
   ],
   "source": [
    "# Test the tokenizer\n",
    "anArticle = dataset['test'][2]\n",
    "\n",
    "print(anArticle['full_article'])\n",
    "\n",
    "\n",
    "res = my_tokenizer.tokenize(anArticle['full_article'])\n",
    "print(len(res))\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "###### End of the Notebook"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hugging-face-learn-mHgvpzAo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
