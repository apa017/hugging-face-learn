{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hugging Face Datasets\n",
    "\n",
    "In this notebook we will learn how to work with Hugging Face datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Existing Datasets\n",
    "\n",
    "By having the name of a dataset, it is possible to load it directly from Hugging Face."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load a dataset\n",
    "dataset = load_dataset(\"fka/awesome-chatgpt-prompts\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we call the dataset we will see that is stored in form of a dictionary, from which we can see:\n",
    "- the name of the features, and\n",
    "- the number of rows or datapoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is possible to explore the dataset by investigating the single rows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['act', 'prompt'],\n",
       "    num_rows: 170\n",
       "})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# First row of the dataset\n",
    "dataset['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Last row of the dataset\n",
    "dataset['train'][-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['act', 'prompt']\n"
     ]
    }
   ],
   "source": [
    "# for a better exploration of the content, you can also list the features\n",
    "feature_names = list(dataset['train'].features)\n",
    "print(feature_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This means we can reverse engineer the data into a pandas dataframe and viceversa\n",
    "import pandas as pd\n",
    "df = pd.DataFrame(columns=feature_names)\n",
    "\n",
    "for feature in feature_names:\n",
    "    df[feature] = [value for value in dataset['train'][feature]]\n",
    "\n",
    "print(f\"Size of DataFrame = {len(df)}\\n\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# and save the data into preferred format like .csv, .json etc\n",
    "print('Storing dataframe as file locally ...\\n')\n",
    "df.to_csv('data/awesome_chatgpt_prompts.csv', encoding='UTF-8', index=False)\n",
    "print('File saved.\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing Method\n",
    "\n",
    "In this section we will preprocess data loaded from Hugging Face.\n",
    "\n",
    "<br>\n",
    "\n",
    "### Shuffling\n",
    "\n",
    "Supposing we want to create a train-test split, we will first **shuffle** the dataset and select a number of samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select 100 random samples (seed = random n. generator) (range = length subset)\n",
    "shuffled_data = dataset['train'].shuffle(seed=42).select(range(100))\n",
    "\n",
    "print(f\"Size shuffled sampled dataset: {len(shuffled_data)}\")\n",
    "\n",
    "shuffled_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train-Test Split\n",
    "\n",
    "A dataset can be divided into two parts, one for training and another one for testing.\n",
    "\n",
    "Most common splits are 80/20 or 70/30 according to size of dataset and purpose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform Train-Test Split (80/20 split)\n",
    "split_data = shuffled_data.train_test_split(train_size=0.8, seed=42)\n",
    "\n",
    "split_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a dataset\n",
    "\n",
    "In this section we will load some unprocessed dataset and prepare for LLM training.\n",
    "\n",
    "**Data Source**: old articles from Reuters\n",
    "\n",
    "Source Link = https://archive.ics.uci.edu/ml/machine-learning-databases/reuters21578-mld/reuters21578.tar.gz "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download the raw data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<i>The command `!wget file_link` allows to download a file locally.</i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://archive.ics.uci.edu/ml/machine-learning-databases/reuters21578-mld/reuters21578.tar.gz "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**UNIX/LINUX ONLY:**\n",
    "\n",
    "The command `!tar -xzvf filename.tar.gz`  is used  to extract the contents of a .tar.gz file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.25s - pydevd: Sending message related to process being replaced timed-out after 5 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x README.txt\n",
      "x all-exchanges-strings.lc.txt\n",
      "x all-orgs-strings.lc.txt\n",
      "x all-people-strings.lc.txt\n",
      "x all-places-strings.lc.txt\n",
      "x all-topics-strings.lc.txt\n",
      "x cat-descriptions_120396.txt\n",
      "x feldman-cia-worldfactbook-data.txt\n",
      "x lewis.dtd\n",
      "x reut2-000.sgm\n",
      "x reut2-001.sgm\n",
      "x reut2-002.sgm\n",
      "x reut2-003.sgm\n",
      "x reut2-004.sgm\n",
      "x reut2-005.sgm\n",
      "x reut2-006.sgm\n",
      "x reut2-007.sgm\n",
      "x reut2-008.sgm\n",
      "x reut2-009.sgm\n",
      "x reut2-010.sgm\n",
      "x reut2-011.sgm\n",
      "x reut2-012.sgm\n",
      "x reut2-013.sgm\n",
      "x reut2-014.sgm\n",
      "x reut2-015.sgm\n",
      "x reut2-016.sgm\n",
      "x reut2-017.sgm\n",
      "x reut2-018.sgm\n",
      "x reut2-019.sgm\n",
      "x reut2-020.sgm\n",
      "x reut2-021.sgm\n"
     ]
    }
   ],
   "source": [
    "!tar -xzvf reuters21578.tar.gz -C extracted_tars"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parse the dataset\n",
    "\n",
    "We can use BeautifulSoup to parse the dataset and make it readable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file: /Users/apavigli/gitrepos/hugging-face-learn/extracted_tars/reut2-004.sgm\n",
      "Processing file: /Users/apavigli/gitrepos/hugging-face-learn/extracted_tars/reut2-010.sgm\n",
      "Processing file: /Users/apavigli/gitrepos/hugging-face-learn/extracted_tars/reut2-011.sgm\n",
      "Processing file: /Users/apavigli/gitrepos/hugging-face-learn/extracted_tars/reut2-005.sgm\n",
      "Processing file: /Users/apavigli/gitrepos/hugging-face-learn/extracted_tars/reut2-013.sgm\n",
      "Processing file: /Users/apavigli/gitrepos/hugging-face-learn/extracted_tars/reut2-007.sgm\n",
      "Processing file: /Users/apavigli/gitrepos/hugging-face-learn/extracted_tars/reut2-006.sgm\n",
      "Processing file: /Users/apavigli/gitrepos/hugging-face-learn/extracted_tars/reut2-012.sgm\n",
      "Processing file: /Users/apavigli/gitrepos/hugging-face-learn/extracted_tars/reut2-016.sgm\n",
      "Processing file: /Users/apavigli/gitrepos/hugging-face-learn/extracted_tars/reut2-002.sgm\n",
      "Processing file: /Users/apavigli/gitrepos/hugging-face-learn/extracted_tars/reut2-003.sgm\n",
      "Processing file: /Users/apavigli/gitrepos/hugging-face-learn/extracted_tars/reut2-017.sgm\n",
      "Processing file: /Users/apavigli/gitrepos/hugging-face-learn/extracted_tars/reut2-001.sgm\n",
      "Processing file: /Users/apavigli/gitrepos/hugging-face-learn/extracted_tars/reut2-015.sgm\n",
      "Processing file: /Users/apavigli/gitrepos/hugging-face-learn/extracted_tars/reut2-014.sgm\n",
      "Processing file: /Users/apavigli/gitrepos/hugging-face-learn/extracted_tars/reut2-000.sgm\n",
      "Processing file: /Users/apavigli/gitrepos/hugging-face-learn/extracted_tars/reut2-019.sgm\n",
      "Processing file: /Users/apavigli/gitrepos/hugging-face-learn/extracted_tars/reut2-018.sgm\n",
      "Processing file: /Users/apavigli/gitrepos/hugging-face-learn/extracted_tars/reut2-020.sgm\n",
      "Processing file: /Users/apavigli/gitrepos/hugging-face-learn/extracted_tars/reut2-008.sgm\n",
      "Processing file: /Users/apavigli/gitrepos/hugging-face-learn/extracted_tars/reut2-009.sgm\n",
      "Processing file: /Users/apavigli/gitrepos/hugging-face-learn/extracted_tars/reut2-021.sgm\n"
     ]
    }
   ],
   "source": [
    "# establish directory where files are stored (if not same as notebook or script)\n",
    "directory = \"/Users/apavigli/gitrepos/hugging-face-learn/extracted_tars/\"\n",
    "\n",
    "\n",
    "# Open file and parse its content\n",
    "articles = []\n",
    "for file_name in os.listdir(directory):\n",
    "    if file_name.endswith('.sgm'):  # Ensure we only process .sgm files\n",
    "        full_path = os.path.join(directory, file_name)  # Create full file path\n",
    "        print(f\"Processing file: {full_path}\")  # Show the file being processed\n",
    "        \n",
    "        try:\n",
    "            with open(full_path, \"r\", encoding=\"latin-1\") as file:\n",
    "                soup = BeautifulSoup(file, \"html.parser\")\n",
    "                articles.append(soup)  # Append parsed content to articles\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing file {full_path}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract articles titles and bodies\n",
    "parsed_articles = []\n",
    "\n",
    "for reuters in soup.find_all('reuters'):\n",
    "    title = reuters.title.string if reuters.title else \"\"\n",
    "    body = reuters.body.string if reuters.body else \"\"\n",
    "    parsed_articles.append(\n",
    "        {\n",
    "            \"title\":title,\n",
    "            \"body\":body\n",
    "        }\n",
    "    )\n",
    "articles.extend(parsed_articles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'parsed_articles' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Print out first few articles for inspection\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, article \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[43mparsed_articles\u001b[49m[:\u001b[38;5;241m5\u001b[39m]):\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;28mprint\u001b[39m(article)\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m-\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m100\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'parsed_articles' is not defined"
     ]
    }
   ],
   "source": [
    "# Print out first few articles for inspection\n",
    "for i, article in enumerate(parsed_articles[:5]):\n",
    "    print(article)\n",
    "    print(\"-\"*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Another way to print the data\n",
    "print(parsed_articles[1]['title'])\n",
    "print(\"-\"*75)\n",
    "print(parsed_articles[1]['body'])\n",
    "print(\"-\"*75)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split the dataset: Train, Test, and Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Establish % of training and validation set\n",
    "TRAIN_PCT, VALID_PCT = 0.8, 0.1\n",
    "total_articles = len(parsed_articles)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split Data\n",
    "train_set = parsed_articles[:int(total_articles * TRAIN_PCT)]\n",
    "valid_set = parsed_articles[int(total_articles * TRAIN_PCT): int(total_articles * (TRAIN_PCT + VALID_PCT))]\n",
    "test_set = parsed_articles[int(total_articles * (TRAIN_PCT + VALID_PCT)):]\n",
    "\n",
    "for set, set_name in zip([train_set, valid_set, test_set], ['train set', 'valid set', 'test set']):\n",
    "    print(f\"Length {set_name} = {len(set)} == {round(len(set) / total_articles, 4)}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save data as JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define directory for output\n",
    "output_dir = \"/Users/apavigli/gitrepos/hugging-face-learn/extracted_json_articles/\"\n",
    "\n",
    "# Helper function\n",
    "def save_as_json(data, filename):\n",
    "    with open(filename, \"w\") as f:\n",
    "        for article in data:\n",
    "            f.write(json.dumps(article) + '\\n')\n",
    "\n",
    "\n",
    "# Save as json\n",
    "save_as_json(train_set, f\"{output_dir}train.json\")\n",
    "save_as_json(valid_set, f\"{output_dir}valid.json\")\n",
    "save_as_json(test_set, f\"{output_dir}test.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load preprocessed dataset from JSON for model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create ingestion training dataset\n",
    "\n",
    "data_files = {\n",
    "    \"train\": f\"{output_dir}train.json\",\n",
    "    \"validation\": f\"{output_dir}valid.json\",\n",
    "    \"test\": f\"{output_dir}test.json\"\n",
    "}\n",
    "\n",
    "dataset = load_dataset(\"json\", data_files=data_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore dataset: General\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore dataset: test set\n",
    "dataset['test'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore dataset: validation set\n",
    "dataset['validation'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore dataset: training set\n",
    "dataset['train'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upload Dataset to GitHub\n",
    "\n",
    "We can upload our dataset to Git Hub hence contributing to the community"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import notebook_login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "notebook_login()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hugging-face-learn-mHgvpzAo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
