{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hugging Face Datasets\n",
    "\n",
    "In this notebook we will learn how to work with Hugging Face datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Existing Datasets\n",
    "\n",
    "By having the name of a dataset, it is possible to load it directly from Hugging Face."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linux/Unix: create subfolders for organizing data\n",
    "\n",
    "!mkdir data\n",
    "!mkdir extracted_json_articles\n",
    "!mkdir extracted_tars\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load a dataset\n",
    "dataset = load_dataset(\"fka/awesome-chatgpt-prompts\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we call the dataset we will see that is stored in form of a dictionary, from which we can see:\n",
    "- the name of the features, and\n",
    "- the number of rows or datapoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['act', 'prompt'],\n",
       "        num_rows: 170\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is possible to explore the dataset by investigating the single rows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['act', 'prompt'],\n",
       "    num_rows: 170\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# First row of the dataset\n",
    "dataset['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'act': 'Architectural Expert',\n",
       " 'prompt': 'I am an expert in the field of architecture, well-versed in various aspects including architectural design, architectural history and theory, structural engineering, building materials and construction, architectural physics and environmental control, building codes and standards, green buildings and sustainable design, project management and economics, architectural technology and digital tools, social cultural context and human behavior, communication and collaboration, as well as ethical and professional responsibilities. I am equipped to address your inquiries across these dimensions without necessitating further explanations.'}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Last row of the dataset\n",
    "dataset['train'][-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['act', 'prompt']\n"
     ]
    }
   ],
   "source": [
    "# for a better exploration of the content, you can also list the features\n",
    "feature_names = list(dataset['train'].features)\n",
    "print(feature_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of DataFrame = 170\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>act</th>\n",
       "      <th>prompt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>An Ethereum Developer</td>\n",
       "      <td>Imagine you are an experienced Ethereum develo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>SEO Prompt</td>\n",
       "      <td>Using WebPilot, create an outline for an artic...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Linux Terminal</td>\n",
       "      <td>I want you to act as a linux terminal. I will ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>English Translator and Improver</td>\n",
       "      <td>I want you to act as an English translator, sp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>`position` Interviewer</td>\n",
       "      <td>I want you to act as an interviewer. I will be...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               act  \\\n",
       "0            An Ethereum Developer   \n",
       "1                       SEO Prompt   \n",
       "2                   Linux Terminal   \n",
       "3  English Translator and Improver   \n",
       "4           `position` Interviewer   \n",
       "\n",
       "                                              prompt  \n",
       "0  Imagine you are an experienced Ethereum develo...  \n",
       "1  Using WebPilot, create an outline for an artic...  \n",
       "2  I want you to act as a linux terminal. I will ...  \n",
       "3  I want you to act as an English translator, sp...  \n",
       "4  I want you to act as an interviewer. I will be...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This means we can reverse engineer the data into a pandas dataframe and viceversa\n",
    "import pandas as pd\n",
    "df = pd.DataFrame(columns=feature_names)\n",
    "\n",
    "for feature in feature_names:\n",
    "    df[feature] = [value for value in dataset['train'][feature]]\n",
    "\n",
    "print(f\"Size of DataFrame = {len(df)}\\n\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Storing dataframe as file locally ...\n",
      "\n",
      "File saved.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# and save the data into preferred format like .csv, .json etc\n",
    "print('Storing dataframe as file locally ...\\n')\n",
    "df.to_csv('data/awesome_chatgpt_prompts.csv', encoding='UTF-8', index=False)\n",
    "print('File saved.\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing Method\n",
    "\n",
    "In this section we will preprocess data loaded from Hugging Face.\n",
    "\n",
    "<br>\n",
    "\n",
    "### Shuffling\n",
    "\n",
    "Supposing we want to create a train-test split, we will first **shuffle** the dataset and select a number of samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size shuffled sampled dataset: 100\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['act', 'prompt'],\n",
       "    num_rows: 100\n",
       "})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Select 100 random samples (seed = random n. generator) (range = length subset)\n",
    "shuffled_data = dataset['train'].shuffle(seed=42).select(range(100))\n",
    "\n",
    "print(f\"Size shuffled sampled dataset: {len(shuffled_data)}\")\n",
    "\n",
    "shuffled_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train-Test Split\n",
    "\n",
    "A dataset can be divided into two parts, one for training and another one for testing.\n",
    "\n",
    "Most common splits are 80/20 or 70/30 according to size of dataset and purpose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['act', 'prompt'],\n",
       "        num_rows: 80\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['act', 'prompt'],\n",
       "        num_rows: 20\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Perform Train-Test Split (80/20 split)\n",
    "split_data = shuffled_data.train_test_split(train_size=0.8, seed=42)\n",
    "\n",
    "split_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a dataset\n",
    "\n",
    "In this section we will load some unprocessed dataset and prepare for LLM training.\n",
    "\n",
    "**Data Source**: old articles from Reuters\n",
    "\n",
    "Source Link = https://archive.ics.uci.edu/ml/machine-learning-databases/reuters21578-mld/reuters21578.tar.gz "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download the raw data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<i>The command `!wget file_link` allows to download a file locally.</i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2024-09-27 16:24:13--  https://archive.ics.uci.edu/ml/machine-learning-databases/reuters21578-mld/reuters21578.tar.gz\n",
      "Resolving archive.ics.uci.edu (archive.ics.uci.edu)... 128.195.10.252\n",
      "Connecting to archive.ics.uci.edu (archive.ics.uci.edu)|128.195.10.252|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: unspecified\n",
      "Saving to: 'reuters21578.tar.gz.1'\n",
      "\n",
      "reuters21578.tar.gz     [  <=>               ]   7.25M   149KB/s               ^C\n"
     ]
    }
   ],
   "source": [
    "!wget https://archive.ics.uci.edu/ml/machine-learning-databases/reuters21578-mld/reuters21578.tar.gz "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**UNIX/LINUX ONLY:**\n",
    "\n",
    "The command `!tar -xzvf filename.tar.gz`  is used  to extract the contents of a .tar.gz file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.25s - pydevd: Sending message related to process being replaced timed-out after 5 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x README.txt\n",
      "x all-exchanges-strings.lc.txt\n",
      "x all-orgs-strings.lc.txt\n",
      "x all-people-strings.lc.txt\n",
      "x all-places-strings.lc.txt\n",
      "x all-topics-strings.lc.txt\n",
      "x cat-descriptions_120396.txt\n",
      "x feldman-cia-worldfactbook-data.txt\n",
      "x lewis.dtd\n",
      "x reut2-000.sgm\n",
      "x reut2-001.sgm\n",
      "x reut2-002.sgm\n",
      "x reut2-003.sgm\n",
      "x reut2-004.sgm\n",
      "x reut2-005.sgm\n",
      "x reut2-006.sgm\n",
      "x reut2-007.sgm\n",
      "x reut2-008.sgm\n",
      "x reut2-009.sgm\n",
      "x reut2-010.sgm\n",
      "x reut2-011.sgm\n",
      "x reut2-012.sgm\n",
      "x reut2-013.sgm\n",
      "x reut2-014.sgm\n",
      "x reut2-015.sgm\n",
      "x reut2-016.sgm\n",
      "x reut2-017.sgm\n",
      "x reut2-018.sgm\n",
      "x reut2-019.sgm\n",
      "x reut2-020.sgm\n",
      "x reut2-021.sgm\n"
     ]
    }
   ],
   "source": [
    "!tar -xzvf reuters21578.tar.gz -C extracted_tars"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parse the dataset\n",
    "\n",
    "We can use BeautifulSoup to parse the dataset and make it readable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file: /Users/apavigli/gitrepos/hugging-face-learn/extracted_tars/reut2-004.sgm\n",
      "Processing file: /Users/apavigli/gitrepos/hugging-face-learn/extracted_tars/reut2-010.sgm\n",
      "Processing file: /Users/apavigli/gitrepos/hugging-face-learn/extracted_tars/reut2-011.sgm\n",
      "Processing file: /Users/apavigli/gitrepos/hugging-face-learn/extracted_tars/reut2-005.sgm\n",
      "Processing file: /Users/apavigli/gitrepos/hugging-face-learn/extracted_tars/reut2-013.sgm\n",
      "Processing file: /Users/apavigli/gitrepos/hugging-face-learn/extracted_tars/reut2-007.sgm\n",
      "Processing file: /Users/apavigli/gitrepos/hugging-face-learn/extracted_tars/reut2-006.sgm\n",
      "Processing file: /Users/apavigli/gitrepos/hugging-face-learn/extracted_tars/reut2-012.sgm\n",
      "Processing file: /Users/apavigli/gitrepos/hugging-face-learn/extracted_tars/reut2-016.sgm\n",
      "Processing file: /Users/apavigli/gitrepos/hugging-face-learn/extracted_tars/reut2-002.sgm\n",
      "Processing file: /Users/apavigli/gitrepos/hugging-face-learn/extracted_tars/reut2-003.sgm\n",
      "Processing file: /Users/apavigli/gitrepos/hugging-face-learn/extracted_tars/reut2-017.sgm\n",
      "Processing file: /Users/apavigli/gitrepos/hugging-face-learn/extracted_tars/reut2-001.sgm\n",
      "Processing file: /Users/apavigli/gitrepos/hugging-face-learn/extracted_tars/reut2-015.sgm\n",
      "Processing file: /Users/apavigli/gitrepos/hugging-face-learn/extracted_tars/reut2-014.sgm\n",
      "Processing file: /Users/apavigli/gitrepos/hugging-face-learn/extracted_tars/reut2-000.sgm\n",
      "Processing file: /Users/apavigli/gitrepos/hugging-face-learn/extracted_tars/reut2-019.sgm\n",
      "Processing file: /Users/apavigli/gitrepos/hugging-face-learn/extracted_tars/reut2-018.sgm\n",
      "Processing file: /Users/apavigli/gitrepos/hugging-face-learn/extracted_tars/reut2-020.sgm\n",
      "Processing file: /Users/apavigli/gitrepos/hugging-face-learn/extracted_tars/reut2-008.sgm\n",
      "Processing file: /Users/apavigli/gitrepos/hugging-face-learn/extracted_tars/reut2-009.sgm\n",
      "Processing file: /Users/apavigli/gitrepos/hugging-face-learn/extracted_tars/reut2-021.sgm\n"
     ]
    }
   ],
   "source": [
    "# establish directory where files are stored (if not same as notebook or script)\n",
    "directory = \"/Users/apavigli/gitrepos/hugging-face-learn/extracted_tars/\"\n",
    "\n",
    "\n",
    "# Open file and parse its content\n",
    "articles = []\n",
    "for file_name in os.listdir(directory):\n",
    "    if file_name.endswith('.sgm'):  # Ensure we only process .sgm files\n",
    "        full_path = os.path.join(directory, file_name)  # Create full file path\n",
    "        print(f\"Processing file: {full_path}\")  # Show the file being processed\n",
    "        \n",
    "        try:\n",
    "            with open(full_path, \"r\", encoding=\"latin-1\") as file:\n",
    "                soup = BeautifulSoup(file, \"html.parser\")\n",
    "                articles.append(soup)  # Append parsed content to articles\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing file {full_path}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract articles titles and bodies\n",
    "parsed_articles = []\n",
    "\n",
    "for reuters in soup.find_all('reuters'):\n",
    "    title = reuters.title.string if reuters.title else \"\"\n",
    "    body = reuters.body.string if reuters.body else \"\"\n",
    "    parsed_articles.append(\n",
    "        {\n",
    "            \"title\":title,\n",
    "            \"body\":body\n",
    "        }\n",
    "    )\n",
    "articles.extend(parsed_articles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'title': 'CITYFED FINANCIAL CORP SAYS IT CUT QTRLY DIVIDEND TO ONE CENT FROM 10 CTS/SHR\\n', 'body': ''}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "{'title': 'HUGE OIL PLATFORMS DOT GULF LIKE BEACONS', 'body': 'Huge oil platforms dot the Gulf like\\nbeacons -- usually lit up like Christmas trees at night.\\n    One of them, sitting astride the Rostam offshore oilfield,\\nwas all but blown out of the water by U.S. Warships on Monday.\\n    The Iranian platform, an unsightly mass of steel and\\nconcrete, was a three-tier structure rising 200 feet (60\\nmetres) above the warm waters of the Gulf until four U.S.\\nDestroyers pumped some 1,000 shells into it.\\n    The U.S. Defense Department said just 10 pct of one section\\nof the structure remained.\\n    U.S. helicopters destroyed three Iranian gunboats after an\\nAmerican helicopter came under fire earlier this month and U.S.\\nforces attacked, seized, and sank an Iranian ship they said had\\nbeen caught laying mines.\\n    But Iran was not deterred, according to U.S. defense\\nofficials, who said Iranian forces used Chinese-made Silkworm\\nmissiles to hit a U.S.-owned Liberian-flagged ship on Thursday\\nand the Sea Isle City on Friday.\\n    Both ships were hit in the territorial waters of Kuwait, a\\nkey backer of Iraq in its war with Iran.\\n    Henry Schuler, a former U.S. diplomat in the Middle East\\nnow with CSIS said Washington had agreed to escort Kuwaiti\\ntankers in order to deter Iranian attacks on shipping.\\n    But he said the deterrence policy had failed and the level\\nof violence and threats to shipping had increased as a result\\nof U.S. intervention and Iran\\'s response.\\n    The attack on the oil platform was the latest example of a\\nU.S. \"tit-for-tat\" policy that gave Iran the initiative, said\\nHarlan Ullman, an ex-career naval officer now with CSIS.\\n    He said with this appraoch America would suffer \"the death\\nof one thousand cuts.\"\\n    But for the United States to grab the initiative\\nmilitarily, it must take warlike steps such as mining Iran\\'s\\nharbors or blockading the mouth of the Gulf through which its\\nshipping must pass, Schuler said.\\n    He was among those advocating mining as a means of bringing\\nIran to the neogtiating table. If vital supplies were cut off,\\nTehran could not continue the war with Iraq.\\n    Ullman said Washington should join Moscow in a diplomatic\\ninitiative to end the war and the superpowers should impose an\\narms embargo against Tehran if it refused to negotiate.\\n    He said the United States should also threaten to mine and\\nblockade Iran if it continued fighting and must press Iraq to\\nacknowledge responsibility for starting the war as part of a\\nsettlement.\\n    Iranian and Western diplomats say Iraq started the war by\\ninvading Iran\\'s territory in 1980. Iraq blames Iran for the\\noutbreak of hostilities, which have entailed World War I-style\\ninfantry attacks resulting in horrific casualties.\\n    Each side has attacked the others\\' shipping.\\n Reuter\\n\\x03'}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "{'title': 'CCR VIDEO SAYST RECEIVED OFFER TO NEGOTIATE A TAKEOVER BY INTERCEP INVESTMENT CORP\\n', 'body': ''}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "{'title': 'GM <GM> CANADA UNIT MAJOR OFFER ACCEPTED BY UNION', 'body': 'The Canadian Auto Workers\\' Union said it\\naccepted an economic offer from the Canadian division of\\nGeneral Motors Corp <GM> in contract negotiations.\\n    But union president Bob White said many local issues at the\\n11 plants in Ontario and Quebec still remained unresolved ahead\\nof Thursday\\'s deadline for a strike by 40,000 workers.\\n    \"It minimizes the possibility of a strike,\" White told\\nreporters.\\n    However, \"if we don\\'t have local agreements settled by\\nThursday, there will be a strike,\" he said.\\n    The local issues still unresolved involved health care,\\nskilled trades and job classifications, White said.\\n    GM Canada negotiator Rick Curd said he believed a strike\\nwould be avoided.\\n    \"Even though there are some tough issues to be resolved\\nwe\\'re on the right schedule to meet the target,\" Curd said.\\n    \"I\\'m very pleased with the state of the negotiations,\" he\\nsaid.\\n    Union membership meetings have been scheduled for the\\nweekend in case a tentative settlement, said White.\\n    White said the union has also received assurances that a\\njob protection pact negotiated with GM workers in the U.S. does\\nnot threaten Canadian jobs.\\n    The economic offer for a three-year pact largely matches\\nagreements at Ford <F> and Chrysler <C> in Canada, which\\ninclude inflation-indexed payments for future retirees and\\nfixed annual payments for current retirees.\\n    It also gives workers wage increases of three pct\\nimmediately and 1.5 pct in each of the second and third years.\\n Reuter\\n\\x03'}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "{'title': 'CANADA DEVELOPMENT UNIT <CDC.TO> REFINANCES', 'body': 'Canada Development Corp said its\\n<Polysar Ltd> unit completed a refinancing package worth about\\n830 mln Canadian dlrs.\\n    The company said the financing, which involves 24 banks \\nand four syndicated loans, consists of a 380 mln Canadian dlr\\nrevolver, a 200 mln Canadian dlr European medium term loan, a\\n149 mln Canadian dlr revolver and a 100 mln Canadian dlr\\noperating loan.\\n    The company said the refinancing will reduce borrowing\\ncosts, in addition to having other benefits.\\n Reuter\\n\\x03'}\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Print out first few articles for inspection\n",
    "for i, article in enumerate(parsed_articles[:5]):\n",
    "    print(article)\n",
    "    print(\"-\"*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HUGE OIL PLATFORMS DOT GULF LIKE BEACONS\n",
      "---------------------------------------------------------------------------\n",
      "Huge oil platforms dot the Gulf like\n",
      "beacons -- usually lit up like Christmas trees at night.\n",
      "    One of them, sitting astride the Rostam offshore oilfield,\n",
      "was all but blown out of the water by U.S. Warships on Monday.\n",
      "    The Iranian platform, an unsightly mass of steel and\n",
      "concrete, was a three-tier structure rising 200 feet (60\n",
      "metres) above the warm waters of the Gulf until four U.S.\n",
      "Destroyers pumped some 1,000 shells into it.\n",
      "    The U.S. Defense Department said just 10 pct of one section\n",
      "of the structure remained.\n",
      "    U.S. helicopters destroyed three Iranian gunboats after an\n",
      "American helicopter came under fire earlier this month and U.S.\n",
      "forces attacked, seized, and sank an Iranian ship they said had\n",
      "been caught laying mines.\n",
      "    But Iran was not deterred, according to U.S. defense\n",
      "officials, who said Iranian forces used Chinese-made Silkworm\n",
      "missiles to hit a U.S.-owned Liberian-flagged ship on Thursday\n",
      "and the Sea Isle City on Friday.\n",
      "    Both ships were hit in the territorial waters of Kuwait, a\n",
      "key backer of Iraq in its war with Iran.\n",
      "    Henry Schuler, a former U.S. diplomat in the Middle East\n",
      "now with CSIS said Washington had agreed to escort Kuwaiti\n",
      "tankers in order to deter Iranian attacks on shipping.\n",
      "    But he said the deterrence policy had failed and the level\n",
      "of violence and threats to shipping had increased as a result\n",
      "of U.S. intervention and Iran's response.\n",
      "    The attack on the oil platform was the latest example of a\n",
      "U.S. \"tit-for-tat\" policy that gave Iran the initiative, said\n",
      "Harlan Ullman, an ex-career naval officer now with CSIS.\n",
      "    He said with this appraoch America would suffer \"the death\n",
      "of one thousand cuts.\"\n",
      "    But for the United States to grab the initiative\n",
      "militarily, it must take warlike steps such as mining Iran's\n",
      "harbors or blockading the mouth of the Gulf through which its\n",
      "shipping must pass, Schuler said.\n",
      "    He was among those advocating mining as a means of bringing\n",
      "Iran to the neogtiating table. If vital supplies were cut off,\n",
      "Tehran could not continue the war with Iraq.\n",
      "    Ullman said Washington should join Moscow in a diplomatic\n",
      "initiative to end the war and the superpowers should impose an\n",
      "arms embargo against Tehran if it refused to negotiate.\n",
      "    He said the United States should also threaten to mine and\n",
      "blockade Iran if it continued fighting and must press Iraq to\n",
      "acknowledge responsibility for starting the war as part of a\n",
      "settlement.\n",
      "    Iranian and Western diplomats say Iraq started the war by\n",
      "invading Iran's territory in 1980. Iraq blames Iran for the\n",
      "outbreak of hostilities, which have entailed World War I-style\n",
      "infantry attacks resulting in horrific casualties.\n",
      "    Each side has attacked the others' shipping.\n",
      " Reuter\n",
      "\u0003\n",
      "---------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Another way to print the data\n",
    "print(parsed_articles[1]['title'])\n",
    "print(\"-\"*75)\n",
    "print(parsed_articles[1]['body'])\n",
    "print(\"-\"*75)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split the dataset: Train, Test, and Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Establish % of training and validation set\n",
    "TRAIN_PCT, VALID_PCT = 0.8, 0.1\n",
    "total_articles = len(parsed_articles)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length train set = 462 == 0.7993%\n",
      "Length valid set = 58 == 0.1003%\n",
      "Length test set = 58 == 0.1003%\n"
     ]
    }
   ],
   "source": [
    "# Split Data\n",
    "train_set = parsed_articles[:int(total_articles * TRAIN_PCT)]\n",
    "valid_set = parsed_articles[int(total_articles * TRAIN_PCT): int(total_articles * (TRAIN_PCT + VALID_PCT))]\n",
    "test_set = parsed_articles[int(total_articles * (TRAIN_PCT + VALID_PCT)):]\n",
    "\n",
    "for set, set_name in zip([train_set, valid_set, test_set], ['train set', 'valid set', 'test set']):\n",
    "    print(f\"Length {set_name} = {len(set)} == {round(len(set) / total_articles, 4)}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save data as JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define directory for output\n",
    "output_dir = \"/Users/apavigli/gitrepos/hugging-face-learn/extracted_json_articles/\"\n",
    "\n",
    "# Helper function\n",
    "def save_as_json(data, filename):\n",
    "    with open(filename, \"w\") as f:\n",
    "        for article in data:\n",
    "            f.write(json.dumps(article) + '\\n')\n",
    "\n",
    "\n",
    "# Save as json\n",
    "save_as_json(train_set, f\"{output_dir}train.json\")\n",
    "save_as_json(valid_set, f\"{output_dir}valid.json\")\n",
    "save_as_json(test_set, f\"{output_dir}test.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load preprocessed dataset from JSON for model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca2c850730904eeaaf9567f08a58c9b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e9d19cd82c146a5bed3ee8de9312354",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "68f672e7ea34431dbbe6f85832bf936b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# create ingestion training dataset\n",
    "\n",
    "data_files = {\n",
    "    \"train\": f\"{output_dir}train.json\",\n",
    "    \"validation\": f\"{output_dir}valid.json\",\n",
    "    \"test\": f\"{output_dir}test.json\"\n",
    "}\n",
    "\n",
    "dataset = load_dataset(\"json\", data_files=data_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['title', 'body'],\n",
       "        num_rows: 462\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['title', 'body'],\n",
       "        num_rows: 58\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['title', 'body'],\n",
       "        num_rows: 58\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Explore dataset: General\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'title': 'BONN MINISTRY HAS NO COMMENT ON BAKER REMARKS',\n",
       " 'body': \"The West German Finance Ministry declined to\\ncomment on weekend criticism by U.S. Treasury Secretary James\\nBaker of recent West German interest rate increases.\\n    Baker said the U.S. Would re-examine the February Louvre\\nAccord to stabilise currencies reached by leading industrial\\ndemocracies. The rise in West Germany short term interest rates\\nwas not in the spirit of an agreement by these nations in\\nWashington, which reaffirmed the Louvre pact, he said.\\n    A Finance Ministry spokesman, asked for an official\\nministry reaction to Baker's remarks, said he could make no\\ncomment.\\n REUTER\\n\\x03\"}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Explore dataset: test set\n",
    "dataset['test'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'title': 'MICROSOFT CORP 1ST QTR SHR 38 CTS VS 29 CTS\\n', 'body': ''}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Explore dataset: validation set\n",
    "dataset['validation'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'title': 'CITYFED FINANCIAL CORP SAYS IT CUT QTRLY DIVIDEND TO ONE CENT FROM 10 CTS/SHR\\n',\n",
       " 'body': ''}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Explore dataset: training set\n",
    "dataset['train'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upload Dataset to GitHub\n",
    "\n",
    "We can upload our dataset to Git Hub hence contributing to the community"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import notebook_login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2cef64de5c154dff9e6491a531a16a76",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Login to personal Hugging Face Hub (via token with writing permission)\n",
    "notebook_login()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "84c5614131ec4d08b2dd5eba83538dd3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac9de0bc83044d93b3ae1525186c4a99",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c4f779a7af8e4fb7aed963820a5eb383",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "59ea47cbf62640399761c4acd0dfe015",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a3e4a364bc4483f838c3d3ddf7d7db8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6821814dfb614ebf8a9f13fe7ef1881d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/datasets/Kain17/reuters_articles/commit/837a2bdf56f2c24eaa581564094f1e447776143f', commit_message='Upload dataset', commit_description='', oid='837a2bdf56f2c24eaa581564094f1e447776143f', pr_url=None, repo_url=RepoUrl('https://huggingface.co/datasets/Kain17/reuters_articles', endpoint='https://huggingface.co', repo_type='dataset', repo_id='Kain17/reuters_articles'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Store dataset on Hugging Face Hub\n",
    "dataset.push_to_hub(\"reuters_articles\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "###### End of the Notebook"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hugging-face-learn-mHgvpzAo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
