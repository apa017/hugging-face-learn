{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Hugging Face\n",
    "In this Notebook we will explore how to use Hugging Face pre-trained models and transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenizers\n",
    "\n",
    "\n",
    "A **tokenizer** is a crucial component in Natural Language Processing (NLP). It converts raw text into tokens, which are the smallest units of text that a machine learning model can understand. \n",
    "\n",
    "A **token** can be a word, subword, character, or even punctuation depending on the tokenizer used.\n",
    "\n",
    "In this example, we're using the Hugging Face **Transformers** library to load a pretrained tokenizer. \n",
    "\n",
    "The tokenizer is associated with the `sshleifer/distilbart-cnn-12-6` model, which is a distilled version of the BART model, optimized.\n",
    "\n",
    "[Link to the Transformer](https://huggingface.co/sshleifer/distilbart-cnn-12-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"sshleifer/distilbart-cnn-12-6\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now a list of raw text inputs, tokenizes them using the specified tokenizer, feed it to the tokenizer.\n",
    "\n",
    "This converts the tokenized data into padded, truncated tensors suitable for PyTorch (\"pt\").\n",
    "\n",
    "The output is a **dictionary of tensors** that contain the tokenized representation of the input sentences, structured in a way that can be fed into a PyTorch model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[    0,   100,   657,  2239, 11991,   328,     2,     1,     1,     1],\n",
      "        [    0,   100,   465,  2239, 33458,   677,  2777,  1202,     4,     2]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n"
     ]
    }
   ],
   "source": [
    "# provide a raw input\n",
    "raw_inputs = [\n",
    "    \"I love learning languages!\",\n",
    "    \"I find learning Slovak language difficult.\"\n",
    "]\n",
    "\n",
    "# convert input\n",
    "inputs = tokenizer(raw_inputs, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "\n",
    "# print inputs\n",
    "print(inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How a tokenizer works\n",
    "\n",
    "You can feed the tokenizer the input directly without specifying parameters.\n",
    "\n",
    "It converts sentences into tokens and then into token ids.\n",
    "\n",
    "This process is made of two parts: **encoding** and **embedding**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [0, 100, 657, 2239, 11991, 328, 2], 'attention_mask': [1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert text to tokens\n",
    "tokenizer('I love learning languages!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some tokenizers (like GPT, BERT, BART) use subword tokenization. \n",
    "\n",
    "It means that for a single word you can have multiple tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'Ġfind', 'Ġlearning', 'ĠSlov', 'ak', 'Ġlanguage', 'Ġdifficult', '.']\n"
     ]
    }
   ],
   "source": [
    "# Not always 1 token = 1 word... \n",
    "tokens = tokenizer.tokenize('I find learning Slovak language difficult.')\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ID \tTOKEN\n",
      "-----------------\n",
      "100\tI\n",
      "465\tĠfind\n",
      "2239\tĠlearning\n",
      "33458\tĠSlov\n",
      "677\tak\n",
      "2777\tĠlanguage\n",
      "1202\tĠdifficult\n",
      "4\t.\n"
     ]
    }
   ],
   "source": [
    "# Convert tokens to ids\n",
    "token_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "\n",
    "print(\"ID \\tTOKEN\\n-----------------\")\n",
    "for i, y in zip(token_ids, tokens):\n",
    "    print(f\"{i}\\t{y}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoding Process\n",
    "\n",
    "The tokenizer allows for the opposite process  of **decoding**.\n",
    "\n",
    "Starting from token ids, the tokenizer can recover the original words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100, 465, 2239, 33458, 677, 2777, 1202, 4]\n",
      "\n",
      "I find learning Slovak language difficult.\n"
     ]
    }
   ],
   "source": [
    "print(token_ids)\n",
    "print()\n",
    "\n",
    "# decode token ids\n",
    "decoded_tokens = tokenizer.decode(token_ids)\n",
    "print(decoded_tokens)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing data for a model\n",
    "\n",
    "Tokenization can be used to prepare lists of token ids as input for a model.\n",
    "\n",
    "It does that by adding necessary special tokens, padding, and attention masks to prepare them for input into a specific model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a BartTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [0, 100, 465, 2239, 33458, 677, 2777, 1202, 4, 2], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n"
     ]
    }
   ],
   "source": [
    "# prepare tokens for model\n",
    "model_preps = tokenizer.prepare_for_model(token_ids)\n",
    "print(model_preps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_ids : [0, 100, 465, 2239, 33458, 677, 2777, 1202, 4, 2]\n",
      "attention_mask : [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "# Structure of preps for model = dictionary\n",
    "for karg in model_preps.keys():\n",
    "    print(f\"{karg} : {model_preps[karg]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformers: Pipelines\n",
    "\n",
    "The Hugging Face allows for different tasks by using the `pipeline` module.\n",
    "\n",
    "Some of the tasks are:\n",
    "\n",
    "- `sentiment-analysis` : performs sentiment analysis\n",
    "- `question-answering` : provide an answer to given question\n",
    "- `summarization` : summarize a given text\n",
    "- `translation_en_to_fr` : example of translation task\n",
    "- `fill-mask` : predict missing words in a sentence\n",
    "\n",
    "In the next example we will use **sentiment analysis**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert/distilbert-base-uncased-finetuned-sst-2-english and revision 714eb0f (https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'label': 'POSITIVE', 'score': 0.9998078942298889},\n",
       " {'label': 'NEGATIVE', 'score': 0.9950483441352844}]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# use sentiment analysis in pipeline\n",
    "classifier = pipeline(\"sentiment-analysis\")\n",
    "\n",
    "#\n",
    "classifier(\n",
    "    [\n",
    "        \"I love learning languages!\",\n",
    "        \"I hate eating junk food!\"\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example we will instead perform **summarization**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to sshleifer/distilbart-cnn-12-6 and revision a4f8f3e (https://huggingface.co/sshleifer/distilbart-cnn-12-6).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'summary_text': ' Odysseus lands on the island of the Cyclopes during his journey home from the Trojan War and, with some of his men, enters a cave filled with provisions . When the giant Polyphemus returns with his flocks, he blocks the entrance with a great stone and, scorning the usual custom of \\xa0hospitality, eats two of the men . The giant kills and eats two more and leaves the cave  to graze his sheep next morning . After the giant returns in the evening, he returns to the cave and eats three more men . Drunk and unwary, the giant asks Odyseus his name, promising him a guest-gift if he answers'}]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"\"\"Odysseus lands on the island of the Cyclopes during his journey home from the Trojan War and, \n",
    "together with some of his men, enters a cave filled with provisions. When the giant Polyphemus returns \n",
    "home with his flocks, he blocks the entrance with a great stone and, scorning the usual custom of \n",
    "hospitality, eats two of the men. Next morning, the giant kills and eats two more and leaves the cave \n",
    "to graze his sheep. After the giant returns in the evening and eats two more of the men, Odysseus offers \n",
    "Polyphemus some strong and undiluted wine given to him earlier on his journey. Drunk and unwary, the \n",
    "giant asks Odysseus his name, promising him a guest-gift if he answers. Odysseus tells him 'Nobody' and \n",
    "Polyphemus promises to eat this \"Nobody\" last of all. With that, he falls into a drunken sleep. \n",
    "Odysseus had meanwhile hardened a wooden stake in the fire and drives it into Polyphemus' eye. \n",
    "When Polyphemus shouts for help from his fellow giants, saying that \"Nobody\" has hurt him, they think \n",
    "Polyphemus is being afflicted by divine power and recommend prayer as the answer. \"\"\"\n",
    "\n",
    "# initialize summarizer\n",
    "summarizer = pipeline(\"summarization\")\n",
    "summarizer([text])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Example Output:\n",
    "\n",
    "    [{'summary_text': ' Odysseus lands on the island of the Cyclopes during his journey home from the Trojan War and, with some of his men, enters a cave filled with provisions . When the giant Polyphemus returns with his flocks, he blocks the entrance with a great stone and, scorning the usual custom of \\xa0hospitality, eats two of the men . The giant kills and eats two more and leaves the cave  to graze his sheep next morning . After the giant returns in the evening, he returns to the cave and eats three more men . Drunk and unwary, the giant asks Odyseus his name, promising him a guest-gift if he answers'}]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pretrained Models\n",
    "\n",
    "\n",
    "A **pretrained model** on Hugging Face refers has already been trained on a large dataset and can be reused for various tasks without needing to train it from scratch. \n",
    "\n",
    "These models allow users to download and apply them directly for specific applications like text classification, sentiment analysis, translation, text generation, or image recognition.\n",
    "\n",
    "##### Key Benefits of Pretrained Models:\n",
    "\n",
    "- **Time-saving**: You don't need to train the model from scratch, saving computation time and resources.\n",
    "- **Performance**: These models are often trained on massive datasets using powerful computing resources, resulting in high accuracy and generalizability.\n",
    "- **Transfer Learning**: You can fine-tune a pretrained model on your specific task with less data, leveraging the knowledge it has already learned.\n",
    "- **Plug-and-play**: using a pretrained mode with a single line of code.\n",
    "\n",
    "<br>\n",
    "\n",
    "<u>In the next steps we will load a model directly from Hugging Face.</u>\n",
    "\n",
    "We will load a pretrained model specifically designed for **sequence classification tasks**, such as sentiment analysis, spam detection, or topic classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify model (assuming tokenizer and decoder are the same)\n",
    "model_name = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "\n",
    "\n",
    "# Load tokenizer from pretrained model\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Load the model\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenized input\n",
    "text = \"\"\"I like learning German language\"\"\"\n",
    "inputs = tokenizer(text, return_tensors='pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SequenceClassifierOutput(loss=None, logits=tensor([[-2.9966,  3.0761]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Output\n",
    "outputs = model(**inputs)\n",
    "outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding\n",
    "\n",
    "Next we will see how model embeddings work. The following code will perform a series of steps:\n",
    "\n",
    "1. It tokenizes the input sentence, padding and truncating it, and returns a PyTorch tensor.\n",
    "2. The tokenized inputs are fed into a model to obtain the output, which includes token embeddings.\n",
    "\n",
    "<hr>\n",
    "\n",
    "#### Outputs:\n",
    "\n",
    "##### Last Hidden State\n",
    "\n",
    "The **shape of the token embeddings**, called the \"last hidden state,\" is printed, showing the dimensions of the token representations.\n",
    "\n",
    "The last hidden state provides contextual embeddings of tokens, which are crucial for understanding the input's meaning and relationships, making them essential for various NLP tasks like classification and generation.\n",
    "\n",
    "##### Shape of Output\n",
    "\n",
    "The **shape of the output** typically follows the format (batch_size, sequence_length, hidden_size), representing the batch size, the number of tokens, and the size of each token's embedding.\n",
    "\n",
    "The shape of the output conveys important information about the batch size, sequence length, and embedding dimensionality, which is necessary for ensuring proper model functioning, optimizing performance, and adapting the model for specific tasks.\n",
    "\n",
    "<br>\n",
    "\n",
    "<hr>\n",
    "\n",
    "\n",
    "We will import the **AutoModel** class from the Hugging Face Transformers library, which allows you to easily load and utilize pre-trained transformer models without specifying the exact architecture. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModel\n",
    "model = AutoModel.from_pretrained(\"distilbert-base-uncased-finetuned-sst-2-english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 7, 768])\n"
     ]
    }
   ],
   "source": [
    "inputs = tokenizer('I love deep learning!', padding=True, truncation=True, return_tensors='pt')\n",
    "\n",
    "outputs = model(**inputs)\n",
    "\n",
    "print(outputs.last_hidden_state.shape) # the token embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hugging-face-learn-mHgvpzAo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
